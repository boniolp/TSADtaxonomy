{
  "name": "Transformer",
  "full_name": "",
  "category": "Reconstruction-based",
  "Dim": "Multivariate",
  "Sup": "Semi-Supervised",
  "Stream": false,
  "year": 2017,
  "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin"],
  "paper": "Attention is all you need",
  "description": "A Transformer is a neural network architecture originally designed for natural language processing that relies on self-attention mechanisms to capture dependencies between elements in a sequence without regard to their distance, making it highly effective for modeling long-range relationships. In time series analysis, this ability has been leveraged to overcome the limitations of recurrent and convolutional models, which struggle with long-term dependencies and scalability. For anomaly detection, Transformers can learn temporal patterns and contextual dependencies across time, enabling them to distinguish between normal and abnormal behaviors in complex, high-dimensional signals.",
  "code": "https://github.com/jadore801120/attention-is-all-you-need-pytorch",
  "url": "https://arxiv.org/pdf/1706.03762",
  "bibtex": "@inproceedings{10.5555/3295222.3295349,author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},title = {Attention is all you need},year = {2017},isbn = {9781510860964},publisher = {Curran Associates Inc.},address = {Red Hook, NY, USA},booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},pages = {6000–6010},numpages = {11},location = {Long Beach, California, USA},series = {NIPS'17}}"
}


